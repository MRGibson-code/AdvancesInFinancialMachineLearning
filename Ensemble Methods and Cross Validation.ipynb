{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 0.4811966952738904\n"
     ]
    }
   ],
   "source": [
    "#comparitive accuracy of bagging classifier vs just one classifier\n",
    "from scipy.special import comb\n",
    "N, p, k = 100, 1./3, 3.\n",
    "p_=0\n",
    "for i in range(0, int(N/k)+1):\n",
    "    p_+=comb(N, i)*p**i*(1-p)**(N-i)\n",
    "print(p, 1-p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three ways of setting up a random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#avgU = average uniqueness between samples\n",
    "avgU = 1.0\n",
    "\n",
    "clf0 = RandomForestClassifier(n_estimators=1000, class_weight='balanced_subsample', criterion='entropy')\n",
    "\n",
    "clf1 = DecisionTreeClassifier(criterion='entropy',max_features='auto', class_weight='balanced')\n",
    "clf1 = BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU)\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample')\n",
    "clf2 = BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_features=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purging overlapping observations in the training dataset\n",
    "def getTrainTimes(t1, testTimes):\n",
    "    '''\n",
    "    t1.index: time when observation started\n",
    "    t1.value: time when observation ended\n",
    "    testTimes: Times of testing observations'''\n",
    "    trn = t1.copy(deep=True)\n",
    "    for i,j in testTimes.iteritems():\n",
    "        df0 = trn[(i<=trn.index)&(trn.index<=j)].index #train starts within test\n",
    "        df1 = trn[(i<=trn)&(trn<=j)].index #train ends within test\n",
    "        df2 = trn[(trn.index<=i)&(j<=trn)].index #train envelops test\n",
    "        trn = trn.drop(df0.union(df1).union(df2))\n",
    "    return trn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbargoTimes(times, pctEmbargo):\n",
    "    #get embargo time for every bar\n",
    "    step = int(times.shape[0]*pctEmbargo)\n",
    "    if step==0:\n",
    "        mbrg = pd.Series(times, index=times)\n",
    "    else:\n",
    "        mbrg = pd.Series(times[step:], index=times[:-step])\n",
    "        mbrg = mbrg.append(pd.Series(times[-1], index=times[-step:]))\n",
    "    return mbrg\n",
    "\n",
    "'''\n",
    "#applying purging to dataset\n",
    "testTimes=pd.Series(mbrg[dt1],index=[dt0]) #include embargo before purge\n",
    "trainTimes = getTrainTimes(t1, testTimes)\n",
    "testTimes=t1.loc[dt0:dt1].index\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV when observations overlap\n",
    "def PurgedKFold(_BaseKFold):\n",
    "    '''\n",
    "    Extend Kfold to work with labels that span intervals\n",
    "    the train is purged of observations overlapping test-label intervals\n",
    "    test set is assumed contiguous (shuffle=False), w/o training examples in between'''\n",
    "    def __init__(self, n_splits=3, t1=None, pctEmbargo=0.):\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError('must be pd series')\n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.t1=t1\n",
    "        self.pctEmbargo=pctEmbargo\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if(X.index==self.t1.index).sum()!=len(self.t1):\n",
    "            raise ValueError('X and thruDateValues must have the same index')\n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0]*self.pctEmbargo)\n",
    "        test_starts=[(i[0], i[-1]+1) for i in np.array_split(np.arrange(X.shape[0]), self.n_splits)]\n",
    "        for i,j in test_starts:\n",
    "            t0 = self.t1.index[i] #start of test set\n",
    "            test_indices = indices[i:j]\n",
    "            maxT1Idx = self.t1.index.searchSorted(self.t1[test_indices].max())\n",
    "            train_indices = self.t1.index.searchSorted(self.t1[self.t1<=t0].index)\n",
    "            train_indices = np.concatenate((train_indices, indices[maxT1Idx+mbrg:]))\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''there are problems with the cross val in sklearn: 1. scoring functions do not know classes_, as a consequence \n",
    "of sklearns reliance on numpy arrays rather than pandas series. 2. cross_val_score will give different results \n",
    "because it passes weights to the fit method, but not to the log loss method\n",
    "below is a fucntion cvsScore to be used in place of cross_val_score for financial applications'''\n",
    "def cvScore(clf, X, y, sample_weight, scoring='neg_log_loss', t1=None, cv=None, cvGen=None,pctEmbargo=None):\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method')\n",
    "    from sklearn.metrics import log_loss, accuracy_score\n",
    "    from clfSequential import PurgedKFold\n",
    "    if cvGen is None:\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo) #purged\n",
    "    scores=[]\n",
    "    for train, test in cvGen.Split(X=X):\n",
    "        fit = clf.fit(X=X.iloc[train, :], y=y.iloc[train],sample_weight=sample_weight.iloc[train].values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob=fit.predict_proba(X.iloc[test, :])\n",
    "            score_=-log_loss(y.iloc[test],prob,sample_weight=sample_weight.iloc[test].values, labels = clf.classes_)\n",
    "        else: \n",
    "            pred = fit.predict(X.iloc[test,:])\n",
    "            score_ = accuracy_score(y.iloc[test],pred,sample_weight=sample_weight.iloc[test].values)\n",
    "        score.append(score_)\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
